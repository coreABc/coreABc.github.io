[{"id":"03a1af7058cec8bb8efe418ed228e157","title":"正则表达式","content":"正则表达式-语法正则表达式是一种有字符和特殊字符组成的模式，用于描述被匹配的文本模式。正则表达式可以在文本中替换、查找、提取和验证特定的模式。\n普通字符普通字符包括没有显式指定为元字符的所有可打印和不可打印字符。这包括所有大写和小写字母、所有数字、所有标点符号和一些其他符号。\n\n\n\n[ABC]\n匹配 […] 中的所有字符，例如 [aeiou] 匹配字符串 “google runoob taobao” 中所有的 e o u a 字母。\n\n\n\n[^ABC]\n匹配除了 […] 中字符的所有字符，例如 [^aeiou] 匹配字符串 “google runoob taobao” 中除了 e o u a 字母的所有字符。\n\n\n[A-Z]\n[A-Z] 表示一个区间，匹配所有大写字母，[a-z] 表示所有小写字母。\n\n\n.\n匹配除换行符（\\n、\\r）之外的任何单个字符，相等于 [^\\n\\r]。\n\n\n[\\s\\S]\n匹配所有。\\s 是匹配所有空白符，包括换行，\\S 非空白符，不包括换行。\n\n\n\\w\n匹配字母、数字、下划线。等价于 [A-Za-z0-9_]\n\n\n","slug":"正则表达式","date":"2024-09-22T07:39:25.198Z","categories_index":"正则","tags_index":"正则","author_index":"麟"},{"id":"557c3a65716469700d953ea750417d1a","title":"排序算法","content":"十大排序算法排序算法可以分为内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。常见的内部排序算法有：插入排序、希尔排序、选择排序、冒泡排序、归并排序、快速排序、堆排序、基数排序等。\n比较排序：冒泡排序、选择排序、插入排序、归并排序、堆排序、快速排序\n非比较排序：计数排序、基数排序、桶排序\n\n关于时间复杂度平方阶 (O(n2)) 排序 各类简单排序：直接插入、直接选择和冒泡排序。\n线性对数阶 (O(nlog2n)) 排序 快速排序、堆排序和归并排序；\nO(n1+§)) 排序，§ 是介于 0 和 1 之间的常数。 希尔排序\n线性阶 (O(n)) 排序 基数排序，此外还有桶、箱排序。\n关于稳定性\n稳定的排序算法：冒泡排序、插入排序、归并排序和基数排序。\n不是稳定的排序算法：选择排序、快速排序、希尔排序、堆排序。\n名词解释：\n\nn：数据规模\nk：”桶”的个数\nIn-place：占用常数内存，不占用额外内存\nOut-place：占用额外内存\n稳定性：排序后 2 个相等键值的顺序和排序之前它们的顺序相同\n\n1、快速排序时间复杂度为 O(nlogn)，空间复杂度为 n(logn)\n快速排序的最坏运行情况是 O(n²)，比如说顺序数列的快排。但它的平摊期望时间是 O(nlogn)，且 O(nlogn) 记号中隐含的常数因子很小，比复杂度稳定等于 O(nlogn) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。\n\n算法步骤\n从数列中挑出一个元素，称为 “基准”（pivot）;\n重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；\n递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序\n\npython代码12345678910111213141516171819202122232425def quickSort(arr, left=None, right=None):    left = 0 if not isinstance(left,(int, float)) else left    right = len(arr)-1 if not isinstance(right,(int, float)) else right    if left &lt; right:        partitionIndex = partition(arr, left, right)        quickSort(arr, left, partitionIndex-1)        quickSort(arr, partitionIndex+1, right)    return arrdef partition(arr, left, right):    #基点开始定义为数列左节点    pivot = left    index = pivot+1    i = index    while  i &lt;= right:        if arr[i] &lt; arr[pivot]:            #用遍历点和位置点交换比基点小的点，            swap(arr, i, index)            index+=1        i+=1    swap(arr,pivot,index-1)    return index-1def swap(arr, i, j):    arr[i], arr[j] = arr[j], arr[i]\n\njava代码12345678910111213141516171819202122232425262728293031323334353637383940public class QuickSort implements IArraySort &#123;    @Override    public int[] sort(int[] sourceArray) throws Exception &#123;        // 对 arr 进行拷贝，不改变参数内容        int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);        return quickSort(arr, 0, arr.length - 1);    &#125;    private int[] quickSort(int[] arr, int left, int right) &#123;        if (left &lt; right) &#123;            int partitionIndex = partition(arr, left, right);            quickSort(arr, left, partitionIndex - 1);            quickSort(arr, partitionIndex + 1, right);        &#125;        return arr;    &#125;    private int partition(int[] arr, int left, int right) &#123;        // 设定基准值（pivot）        int pivot = left;        int index = pivot + 1;        for (int i = index; i &lt;= right; i++) &#123;            if (arr[i] &lt; arr[pivot]) &#123;                swap(arr, i, index);                index++;            &#125;        &#125;        swap(arr, pivot, index - 1);        return index - 1;    &#125;    private void swap(int[] arr, int i, int j) &#123;        int temp = arr[i];        arr[i] = arr[j];        arr[j] = temp;    &#125;&#125;\n\n\n\n2、冒泡排序时间复杂度O(n^2^) ， 空间复杂度n(1)，正序数组最快，反序数组最慢\n算法步骤比较相邻的元素。如果第一个比第二个大，就交换他们两个。\n对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。\n针对所有的元素重复以上的步骤，除了最后一个。\n持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。\n动图演示\njava代码12345678910111213141516171819202122232425262728public class BubbleSort implements IArraySort &#123;    @Override    public int[] sort(int[] sourceArray) throws Exception &#123;        // 对 arr 进行拷贝，不改变参数内容        int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);        for (int i = 1; i &lt; arr.length; i++) &#123;            // 设定一个标记，若为true，则表示此次循环没有进行交换，也就是待排序列已经有序，排序已经完成。            boolean flag = true;            for (int j = 0; j &lt; arr.length - i; j++) &#123;                if (arr[j] &gt; arr[j + 1]) &#123;                    int tmp = arr[j];                    arr[j] = arr[j + 1];                    arr[j + 1] = tmp;                    flag = false;                &#125;            &#125;            if (flag) &#123;                break;            &#125;        &#125;        return arr;    &#125;&#125;\n\npython代码123456def bubbleSort(arr):    for i in range(1, len(arr)):        for j in range(0, len(arr)-i):            if arr[j] &gt; arr[j+1]:                arr[j], arr[j + 1] = arr[j + 1], arr[j]    return arr\n\n\n\n3、选择排序时间复杂度O(n^2^)，空间复杂度n(1)\n算法步骤首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。\n再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。\n重复第二步，直到所有元素均排序完毕。\n动图演示\njava代码1234567891011121314151617181920212223242526272829public class SelectionSort implements IArraySort &#123;    @Override    public int[] sort(int[] sourceArray) throws Exception &#123;        int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);        // 总共要经过 N-1 轮比较        for (int i = 0; i &lt; arr.length - 1; i++) &#123;            int min = i;            // 每轮需要比较的次数 N-i            for (int j = i + 1; j &lt; arr.length; j++) &#123;                if (arr[j\t] &lt; arr[min]) &#123;                    // 记录目前能找到的最小值元素的下标                    min = j;                &#125;            &#125;            // 将找到的最小值和i位置所在的值进行交换            if (i != min) &#123;                int tmp = arr[i];                arr[i] = arr[min];                arr[min] = tmp;            &#125;        &#125;        return arr;    &#125;&#125;\n\n\n\npython代码1234567891011def selectionSort(arr):    for i in r\tange(len(arr) - 1):        # 记录最小数的索引        minIndex = i        for j in range(i + 1, len(arr)):            if arr[j] &lt; arr[minIndex]:                minIndex = j        # i 不是最小数时，将 i 和最小数进行交换        if i != minIndex:            arr[i], arr[minIndex] = arr[minIndex], arr[i]    return arr\n\n\n\n4、插入排序时间复杂度O(n^2^)，空间复杂度n(1)\n动图演示\n算法步骤将第一待排序序列第一个元素看做一个有序序列，把第二个元素到最后一个元素当成是未排序序列。\n从头到尾依次扫描未排序序列，将扫描到的每个元素插入有序序列的适当位置。（如果待插入的元素与有序序列中的某个元素相等，则将待插入元素插入到相等元素的后面。）\n1234567891011121314151617181920212223242526272829public class InsertSort implements IArraySort &#123;    @Override    public int[] sort(int[] sourceArray) throws Exception &#123;        // 对 arr 进行拷贝，不改变参数内容        int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);        // 从下标为1的元素开始选择合适的位置插入，因为下标为0的只有一个元素，默认是有序的        for (int i = 1; i &lt; arr.length; i++) &#123;            // 记录要插入的数据            int tmp = arr[i];            // 从已经排序的序列最右边的开始比较，找到比其小的数            int j = i;            while (j &gt; 0 &amp;&amp; tmp &lt; arr[j - 1]) &#123;                arr[j] = arr[j - 1];                j--;            &#125;            // 存在比其小的数，插入            if (j != i) &#123;                arr[j] = tmp;            &#125;        &#125;        return arr;    &#125;&#125;\n\n\n\npython代码123456789def insertionSort(arr):    for i in range(len(arr)):        preIndex = i-1        current = arr[i]        while preIndex &gt;= 0 and arr[preIndex] &gt; current:            arr[preIndex+1] = arr[preIndex]            preIndex-=1        arr[preIndex+1] = current    return arr\n\n\n\n5、希尔排序希尔排序的基本思想是：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录”基本有序”时，再对全体记录进行依次直接插入排序。\n算法步骤选择一个增量序列 t1，t2，……，tk，其中 ti &gt; tj, tk &#x3D; 1；\n按增量序列个数 k，对序列进行 k 趟排序；\n每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。\n动图演示\njava代码123456789101112131415public static void shellSort(int[] arr) &#123;    int length = arr.length;    int temp;    for (int step = length / 2; step &gt;= 1; step /= 2) &#123;        for (int i = step; i &lt; length; i++) &#123;            temp = arr[i];            int j = i - step;            while (j &gt;= 0 &amp;&amp; arr[j] &gt; temp) &#123;                arr[j + step] = arr[j];                j -= step;            &#125;            arr[j + step] = temp;        &#125;    &#125;&#125;\n\n\n\npython代码123456789101112131415def shellSort(arr):    import math    gap=1    while(gap &lt; len(arr)/3):        gap = gap*3+1    while gap &gt; 0:        for i in range(gap,len(arr)):            temp = arr[i]            j = i-gap            while j &gt;=0 and arr[j] &gt; temp:                arr[j+gap]=arr[j]                j-=gap            arr[j+gap] = temp        gap = math.floor(gap/3)    return arr\n\n\n\n6、归并排序时间复杂度O(nlogn)，空间复杂度O(n)，out-place\n归并排序（Merge sort）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。\n作为一种典型的分而治之思想的算法应用，归并排序的实现由两种方法：\n\n自上而下的递归（所有递归的方法都可以用迭代重写，所以就有了第 2 种方法）；\n自下而上的迭代；\n\n算法步骤\n申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列；\n设定两个指针，最初位置分别为两个已经排序序列的起始位置；\n比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置；\n重复步骤 3 直到某一指针达到序列尾；\n将另一序列剩下的所有元素直接复制到合并序列尾。\n\n动图演示\njava代码123456789101112131415161718192021222324252627282930313233343536373839404142434445public class MergeSort implements IArraySort &#123;    @Override    public int[] sort(int[] sourceArray) throws Exception &#123;        // 对 arr 进行拷贝，不改变参数内容        int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);        if (arr.length &lt; 2) &#123;            return arr;        &#125;        int middle = (int) Math.floor(arr.length / 2);        int[] left = Arrays.copyOfRange(arr, 0, middle);        int[] right = Arrays.copyOfRange(arr, middle, arr.length);        return merge(sort(left), sort(right));    &#125;    protected int[] merge(int[] left, int[] right) &#123;        int[] result = new int[left.length + right.length];        int i = 0;        while (left.length &gt; 0 &amp;&amp; right.length &gt; 0) &#123;            if (left[0] &lt;= right[0]) &#123;                result[i++] = left[0];                left = Arrays.copyOfRange(left, 1, left.length);            &#125; else &#123;                result[i++] = right[0];                right = Arrays.copyOfRange(right, 1, right.length);            &#125;        &#125;        while (left.length &gt; 0) &#123;            result[i++] = left[0];            left = Arrays.copyOfRange(left, 1, left.length);        &#125;        while (right.length &gt; 0) &#123;            result[i++] = right[0];            right = Arrays.copyOfRange(right, 1, right.length);        &#125;        return result;    &#125;&#125;\n\n\n\npython代码12345678910111213141516171819202122def mergeSort(arr):    import math    if(len(arr)&lt;2):        return arr    middle = math.floor(len(arr)/2)    left, right = arr[0:middle], arr[middle:]    return merge(mergeSort(left), mergeSort(right))def merge(left,right):    result = []    while left and right:        if left[0] &lt;= right[0]:            result.append(left.pop(0))        else:            #pop the first one in the list            result.append(right.pop(0));    while left:        result.append(left.pop(0))    while right:        result.append(right.pop(0));    return result\n\n\n\n7、堆排序时间复杂度O(nlogn), O(1)\n堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。堆排序可以说是一种利用堆的概念来排序的选择排序。分为两种方法：\n\n大顶堆：每个节点的值都大于或等于其子节点的值，在堆排序算法中用于升序排列；\n小顶堆：每个节点的值都小于或等于其子节点的值，在堆排序算法中用于降序排列；\n\n\n堆排序的平均时间复杂度为 Ο(nlogn)。\n算法步骤\n创建一个堆 H[0……n-1]；\n把堆首（最大值）和堆尾互换；\n把堆的尺寸缩小 1，并调用 shift_down(0)，目的是把新的数组顶端数据调整到相应位置；\n重复步骤 2，直到堆的尺寸为 1。\n\n动图演示\n\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class HeapSort implements IArraySort &#123;    @Override    public int[] sort(int[] sourceArray) throws Exception &#123;        // 对 arr 进行拷贝，不改变参数内容        int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);        int len = arr.length;        buildMaxHeap(arr, len);        for (int i = len - 1; i &gt; 0; i--) &#123;            swap(arr, 0, i);            len--;            heapify(arr, 0, len);        &#125;        return arr;    &#125;    private void buildMaxHeap(int[] arr, int len) &#123;        for (int i = (int) Math.floor(len / 2); i &gt;= 0; i--) &#123;            heapify(arr, i, len);        &#125;    &#125;    private void heapify(int[] arr, int i, int len) &#123;        int left = 2 * i + 1;        int right = 2 * i + 2;        int largest = i;        if (left &lt; len &amp;&amp; arr[left] &gt; arr[largest]) &#123;            largest = left;        &#125;        if (right &lt; len &amp;&amp; arr[right] &gt; arr[largest]) &#123;            largest = right;        &#125;        if (largest != i) &#123;            swap(arr, i, largest);            heapify(arr, largest, len);        &#125;    &#125;    private void swap(int[] arr, int i, int j) &#123;        int temp = arr[i];        arr[i] = arr[j];        arr[j] = temp;    &#125;&#125;\n\n123456789101112131415161718192021222324252627282930def buildMaxHeap(arr):    import math    for i in range(math.floor(len(arr)/2),-1,-1):        heapify(arr,i)def heapify(arr, i):    left = 2*i+1    right = 2*i+2    largest = i    if left &lt; arrLen and arr[left] &gt; arr[largest]:        largest = left    if right &lt; arrLen and arr[right] &gt; arr[largest]:        largest = right    if largest != i:        swap(arr, i, largest)        heapify(arr, largest)def swap(arr, i, j):    arr[i], arr[j] = arr[j], arr[i]def heapSort(arr):    global arrLen    arrLen = len(arr)    buildMaxHeap(arr)    for i in range(len(arr)-1,0,-1):        swap(arr,0,i)        arrLen -=1        heapify(arr, 0)    return arr\n\n","slug":"排序算法","date":"2024-09-22T07:39:25.198Z","categories_index":"算法","tags_index":"算法","author_index":"麟"},{"id":"75013197abd18a8eb6035239da3363f3","title":"Tensorflow环境安装的问题","content":"Tensorflow安装踩的坑\n\n\n\n环境\ntensorflow-cpu&#x3D;&#x3D;2.x\ntensorflow&#x3D;&#x3D;2.x\n\n\n\n只有GPU\ncpu运行\ncpu运行\n\n\n有GPU且安装Cuda和Cudnn\ncpu运行\ngpu运行\n\n\n有GPU未安装Cuda和Cudnn\ncpu运行\ncpu运行\n\n\n在tensorflow 2.x后不再区分是否有gpu，当检测到gpu并安装cuda后，自动调用gpu  \n但是，有些人不需要或没有gpu，gpu适配对这部分群体是浪费的（占用不必要的资源），于是有了tensorflow-cpu，我们可以理解其为cpu only版本\n","slug":"Tensorflow安装踩的坑","date":"2024-09-22T07:39:25.197Z","categories_index":"深度学习","tags_index":"Tensorflow","author_index":"麟"},{"id":"43844437b44d7679c1c7503d8f669d76","title":"深度学习数据集","content":"CIFAR-10使用简介 CIFAR-10是一个更接近普适物体的彩色图像数据集。CIFAR-10 是由Hinton 的学生Alex Krizhevsky 和Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含10 个类别的RGB 彩色图片：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。   \n每个图片的尺寸为32 × 32 ，每个类别有6000个图像，数据集中一共有50000 张训练图片和10000 张测试图片。\n\n\nmeta文件中包含每个batch中的用例数量（num_cases_per_batch），标签含义（label_names），每张图片的大小（num_vis,32*32*3&#x3D;3072）\n每个batch文件中包含每个batch的标签即在batch1中batch_label(b’training batch 1 of 5’)，label(0-9),每个数字表示一种类，图片的numpy数据(data)，图片的名称。\n","slug":"CIFAR-10使用简介","date":"2024-09-22T07:39:25.197Z","categories_index":"深度学习数据","tags_index":"数据集","author_index":"麟"},{"id":"b8515575cf4c927344ce80d4d1dec162","title":"VGG16深度学习框架","content":"123456from keras.applications.vgg16 import VGG16# 下载VGG16模型，下载地址为 c:\\user(用户)\\.keras\\models\\vgg16_weights_tf_dim_ordering_tf_kernels.h5model = VGG16(weights=&#x27;imagenet&#x27;, include_top=False)# 显示模型结构model.summary()\n\n可以使用keras.applications 模块进行导入重要参数：\n\ninclude_top ：是否包含顶端的全连接层\n\n\n\n\n\n\n\n\n\n\ninclude_top的作用：\n这些模型中的大多数是一系列卷积层，后跟一个或几个密集（或完全连接）层。\nInclude_top允许您选择是否需要最终的密集层。\n\n卷积层用作特征提取器。它们识别图像中的一系列图案，每一层都可以通过查看图案的图案来识别更复杂的图案。\n密集层能够解释发现的模式以进行分类：此图像包含猫、狗、汽车等。\n\n关于权重：\n\n卷积层中的权重是固定大小的。它们是内核 x 过滤器的大小。示例：包含 3 个筛选器的 3x10 内核。卷积层不关心输入图像的大小。它只是进行卷积，并根据输入图像的大小呈现结果图像。（如果不清楚，请搜索一些关于卷积的图解教程）\n现在，密集层中的权重完全取决于输入大小。它是输入的每个元素一个权重。因此，这要求你的输入始终是相同的大小，否则你将没有适当的学习权重。\n\n因此，删除最终的密集层允许您定义输入大小（请参阅文档中）。（输出大小将相应增加&#x2F;减少）。\n但是您将丢失解释&#x2F;分类图层。（您可以添加自己的任务，具体取决于您的任务）\n\nweight：None代表随机，imagenet初始化，代表加载在ImageNet上预训练的权值\ninput_tensor：可选，Keras张量作为模型的输入（即layers.Input()输出的tensor）。\ninput_shape：可选，输入尺寸元组，当仅include_top&#x3D;False时有效值（否则输入形状必须是(299, 299, 3)，因为预训练模型是以这个大小训练的）它必须拥有3个输入通道，且宽高必须不小于71例如。(150, 150, 3)是一个合法的输入尺寸。\npooling：可选，当include_top为False时，该参数指定了特征提取时的池化方式。\nNone 代表不池化，直接输出最后一层卷积层的输出，该输出是一个4D张量。\n‘avg’ 代表平均值平均池化（GlobalAveragePooling2D），相当于在最后一层卷积层后面再加一层平均池化层，输出是一个2D张量。\n‘max’ 代表最大池化。\n\n\nclasses：可选，图片分类的类别数，仅当include_top为和True不加载预训练权值时可用。\n\n构建完整的模型构建序列模型 &#x3D;&#x3D;&gt; 添加VGG16模型（输入） &#x3D;&#x3D;&gt; 添加全局平均化层 &#x3D;&#x3D;&gt; 添加全连接层（输出）\n123456# 构建模型，增加全连接层model = keras.Sequential()model.add(conv_base)model.add(keras.layers.GlobalAveragePooling2D())model.add(keras.layers.Dense(512, activation=&#x27;relu&#x27;))model.add(keras.layers.Dense(1, activation=&#x27;sigmoid’))\n\n\n\n\n\n\n\n\n\n\n12345678910111213tf.keras.layers.Dense(    units,                                 # 正整数，输出空间的维数    activation=None,                       # 激活函数，不指定则没有    use_bias=True,\t\t\t\t\t\t   # 布尔值，是否使用偏移向量    kernel_initializer=&#x27;glorot_uniform&#x27;,   # 核权重矩阵的初始值设定项    bias_initializer=&#x27;zeros&#x27;,              # 偏差向量的初始值设定项    kernel_regularizer=None,               # 正则化函数应用于核权矩阵    bias_regularizer=None,                 # 应用于偏差向量的正则化函数    activity_regularizer=None,             # Regularizer function applied to the output of the layer (its &quot;activation&quot;)    kernel_constraint=None,                # Constraint function applied to the kernel weights matrix.    bias_constraint=None, **kwargs         # Constraint function applied to the bias vector)\nkeras.Sequential()：建立 Sequential 模型，Sequential 是 Keras 中的一种神经网络框架，可以被认为是一个容器，其中封装了神经网络的结构。Sequential 模型只有一组输入和一组输出。各层之间按照先后顺序进行堆叠。前面一层的输出就是后面一次的输入。通过不同层的堆叠，构建出神经网络。\nGlobalAveragePooling2D()：是平均池化的一个特例，它不需要指定pool_size和strides等参数，操作的实质是将输入特征图的每一个通道求平均得到一个数值。它的输入和输出维度为：\n1234567891011Input shape:    - If `data_format=&#x27;channels_last&#x27;`:        4D tensor with shape:        `(batch_size, rows, cols, channels)`    - If `data_format=&#x27;channels_first&#x27;`:        4D tensor with shape:        `(batch_size, channels, rows, cols)`Output shape:    2D tensor with shape:    `(batch_size, channels)`\n\nactivation：激活函数，无则没有\n\n\n\n\n\n\n\n\n\n激活函数\n激活函数（Activation Function）是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。在神经元中，输入的input经过一系列加权求和后作用于另一个函数，这个函数就是这里的激活函数。类似于人类大脑中基于神经元的模型，激活函数最终决定了是否传递信号以及要发射给下一个神经元的内容。在人工神经网络中，一个节点的激活函数定义了该节点在给定的输入或输入集合下的输出。\n激活函数可以分为线性激活函数（线性方程控制输入到输出的映射，如f(x)&#x3D;x等）以及非线性激活函数（非线性方程控制输入到输出的映射，比如Sigmoid、Tanh、ReLU、LReLU、PReLU、Swish 等）\n\n为什么要使用激活函数\n\n\n\n\n\n\n\n\n\n因为神经网络中每一层的输入输出都是一个线性求和的过程，下一层的输出只是承接了上一层输入函数的线性变换，所以如果没有激活函数，那么无论你构造的神经网络多么复杂，有多少层，最后的输出都是输入的线性组合，纯粹的线性组合并不能够解决更为复杂的问题。而引入激活函数之后，我们会发现常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。\n\n\n常见的激活函数：\n\nsigmoid函数\n\nSigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。sigmoid是一个十分常见的激活函数，函数的表达式如下：\n​\t$$f(x)&#x3D;\\frac{1}{1+e^x}$$在什么情况下适合使用 Sigmoid 激活函数呢？\n\nSigmoid 函数的输出范围是 0 到 1。由于输出值限定在 0 到1，因此它对每个神经元的输出进行了归一化；\n用于将预测概率作为输出的模型。由于概率的取值范围是 0 到 1，因此 Sigmoid 函数非常合适；\n梯度平滑，避免「跳跃」的输出值；\n函数是可微的。这意味着可以找到任意两个点的 sigmoid 曲线的斜率；\n明确的预测，即非常接近 1 或 0。\n\nSigmoid 激活函数存在的不足：\n\n梯度消失：注意：Sigmoid 函数趋近 0 和 1 的时候变化率会变得平坦，也就是说，Sigmoid 的梯度趋近于 0。神经网络使用 Sigmoid 激活函数进行反向传播时，输出接近 0 或 1 的神经元其梯度趋近于 0。这些神经元叫作饱和神经元。因此，这些神经元的权重不会更新。此外，与此类神经元相连的神经元的权重也更新得很慢。该问题叫作梯度消失。因此，想象一下，如果一个大型神经网络包含 Sigmoid 神经元，而其中很多个都处于饱和状态，那么该网络无法执行反向传播。\n不以零为中心：Sigmoid 输出不以零为中心的,，输出恒大于0，非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢。\n计算成本高昂：exp() 函数与其他非线性激活函数相比，计算成本高昂，计算机运行起来速度较慢。\n\n\nReLU激活函数\n\nReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题，在目前的深度神经网络中被广泛使用。ReLU函数本质上是一个斜坡（ramp）函数，公式及函数图像如下：$$f(x)&#x3D;\\begin{cases}x, x\\geqslant0\\0, x\\leqslant0\\end{cases}&#x3D;max(0,x)$$\n全连接层的作用\n全连接层在整个网络卷积神经网络中起到“特征提取器”的作用。如果说卷积层、池化层和激活函数等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的特征表示映射到样本的标记空间的作用。\n一段来自知乎的通俗理解：\n从卷积网络谈起，卷积网络在形式上有一点点像咱们正在召开的“人民代表大会”。卷积核的个数相当于候选人，图像中不同的特征会激活不同的“候选人”（卷积核）。池化层（仅指最大池化）起着类似于“合票”的作用，不同特征在对不同的“候选人”有着各自的喜好。\n全连接相当于是“代表普选”。所有被各个区域选出的代表，对最终结果进行“投票”，全连接保证了receiptive field 是整个图像，既图像中各个部分（所谓所有代表），都有对最终结果影响的权利。\n全连接层的原理\n在卷积神经网络的最后，往往会出现一两层全连接层，全连接一般会把卷积输出的二维特征图转化成一维的一个向量，这是怎么来的呢？目的何在呢？\n\n最后的两列小圆球就是两个全连接层的输出，在最后一层卷积结束后，进行了最后一次池化，得到20个12*12的图像，经过全连接层变成了1*100的向量，再次经过一次全连接层变成的1*10的向量输出。\n从第一步是如何到达第三步的呢，其实就是有20*100个12*12的不同卷积核卷积出来的，我们也可以这样想，就是每个神经元的输出是12*12*20个输入值与对应的权值乘积的和。对于输入的每一张图，用了一个和图像一样大小的核卷积，这样整幅图就变成了一个数了，如果厚度是20就是那20个核卷积完了之后相加求和。这样就能把一张图高度浓缩成一个数了。\n\n模型的初步训练训练步骤\n在预训练卷积上添加自定义层\n使用conv_base.trainable = False冻结卷积基所有层\n训练添加的分类层\n解冻卷积基的一部分层\n联合训练解冻的卷积层和添加的自定义层\n\n在初步训练中要锁定卷积基的参数值，因为我们自己加入的全连接层的参数是随机初始化的，在初步训练中会影响卷积基的参数，导致降低准确率。\n1234567891011121314151617181920conv_base.trainable = False  # 使得VGG卷积中的参数不可训练# 因为在训练模型是，最后的两层全连接层是随机初始化的参数，有可能会影响到VGG16卷积层的参数# 优化模型model.compile(optimizer=keras.optimizers.Adam(lr=0.001),             loss=&#x27;binary_crossentropy&#x27;,             metrics=[&#x27;acc’])# 进行训练history = model.fit(    train_image_ds,                        steps_per_epoch=train_count//BATCH_SIZE,                        epochs=5,                        validation_data=test_image_ds,                        validation_steps=test_count//BATCH_SIZE                    )&gt;&gt;&gt;Train for 62 steps, validate for 31 stepsEpoch 1/562/62 [==============================] - 407s 7s/step - loss: 0.6301 - acc: 0.6190 - val_loss: 0.5986 - val_acc: 0.6552...\n\n\n\n\n\n\n\n\n\n\nmodel.compile(optimizer &#x3D; 优化器，\n​            loss &#x3D; 损失函数，\n​            metrics &#x3D; [“准确率”])\n其中：\noptimizer可以是字符串形式给出的优化器名字，也可以是函数形式，使用函数形式可以设置学习率、动量和超参数\n例如：sgd或者tf.optimizers.SGD(lr=学习率,decay=学习衰减率,momentum=动量参数)\nadagrad或者tf.keras.optimizers.Adagrad(lr=学习率,decay=学习衰减率)\nadadelta或者 tf.keras.optimizers.Adadelta(lr=学习率,decay=学习衰减率)\nadam或者  tf.keras.optimizers.Adam(lr=学习率,decay=学习衰减率)\nloss可以是字符串形式给出的损失函数的名字，也可以是函数形式\n例如：msc或者tf.keras.losses.MeanSquaredError()\nsparse_categorical_crossentropy 或者 tf.keras.losses.SparseCatagoricalCrossentropy(from_logits = False)损失函数经常需要使用softmax函数来将输出转化为概率分布的形式，在这里from_logits代表是否将输出转为概率分布的形式，为False时表示转换为概率分布，为True时表示不转换，直接输出\nMetrics标注网络评价指标\n例如：accuracy：y_ 和 y 都是数值，如y_ &#x3D; [1] y &#x3D; [1] #y_为真实值，y为预测值\nsparse_accuracy：y_和y都是以独热码 和概率分布表示，如y_ &#x3D; [0, 1, 0], y &#x3D; [0.256, 0.695, 0.048]\nsparse_categorical_accuracy：y_是以数值形式给出，y是以独热码给出，如y_ &#x3D; [1], y &#x3D; [0.256 0.695, 0.048]\n\n1、输入224x224x3的图片，经64个3x3的卷积核作两次卷积+ReLU，卷积后的尺寸变为224x224x642、作max pooling（最大化池化），池化单元尺寸为2x2（效果为图像尺寸减半），池化后的尺寸变为112x112x643、经128个3x3的卷积核作两次卷积+ReLU，尺寸变为112x112x1284、作2x2的max pooling池化，尺寸变为56x56x1285、经256个3x3的卷积核作三次卷积+ReLU，尺寸变为56x56x2566、作2x2的max pooling池化，尺寸变为28x28x2567、经512个3x3的卷积核作三次卷积+ReLU，尺寸变为28x28x5128、作2x2的max pooling池化，尺寸变为14x14x5129、经512个3x3的卷积核作三次卷积+ReLU，尺寸变为14x14x51210、作2x2的max pooling池化，尺寸变为7x7x51211、与两层1x1x4096，一层1x1x1000进行全连接+ReLU（共三层）12、通过softmax输出1000个预测结果\n","slug":"VGG学习","date":"2024-09-22T07:39:25.197Z","categories_index":"深度学习","tags_index":"VGG16","author_index":"麟"},{"id":"665540b83d48c62e220c6e1d58a9965e","title":"卷积神经网络","content":"卷积神经网络CNN\n\n\n\n在计算机中图像由像素点组成，通过逐一对照来进行图像匹配的话就如上图所示，the result is so unreasonable，我们希望，对于那些仅仅只是做了一些像平移，缩放，旋转，微变形等简单变换的图像，计算机仍然能够识别出图中的”X”和”O”。就像下面这些情况，我们希望计算机依然能够很快并且很准的识别出来：\n\n\n这也就是CNN需要做的事情  \n\n\n对于CNN，他是一块一块逐一比较的，这个比较的小块我们称之为Feature（特征），在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够看到更好的相似性。  \n每一个feature就像是一个小图（就是一个比较小的有值的二维数组）。不同的Feature匹配图像中不同的特征。在字母”X”的例子中，那些由对角线和交叉线组成的features基本上能够识别出大多数”X”所具有的重要特征。\n\n\n将这些features和原图进行卷积来得到特征矩阵。\n\n\n\n\n\n\n\n\n\n卷积操作：\n\n\n当给你一张新的图时，CNN并不能准确地知道这些features到底要匹配原图的哪些部分，所以它会在原图中每一个可能的位置进行尝试。这样在原始整幅图上每一个位置进行匹配计算，我们相当于把这个feature变成了一个过滤器。这个我们用来匹配的过程就被称为卷积操作，这也就是卷积神经网络名字的由来。\n这个卷积操作背后的数学知识其实非常的简单。要计算一个feature和其在原图上对应的某一小块的结果，只需要简单地将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可。如果两个像素点都是白色（也就是值均为1），那么11 &#x3D; 1，如果均为黑色，那么(-1)(-1) &#x3D; 1。不管哪种情况，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。如果一个feature（比如nn）内部所有的像素都和原图中对应一小块（nn）匹配上了，那么它们对应像素值相乘再累加就等于n2，然后除以像素点总个数n2，结果就是1。同理，如果每一个像素都不匹配，那么结果就是-1。\n最后整张图计算完的样子：\n\n\n当换用其它的feature进行同样的操作，最后的得到的结果：\n\n\n为了完成我们的卷积，我们不断地重复着上述过程，将feature和图中每一块进行卷积操作。最后通过每一个feature的卷积操作，我们会得到一个新的二维数组。这也可以理解为对原始图像进行过滤的结果，我们称之为feature map，它是每一个feature从原始图像中提取出来的“特征”。其中的值，越接近为1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。\n这样我们的原始图，经过不同的feature卷积操作就变成了一系列的feature map\n","slug":"卷积神经网络CNN","date":"2024-09-22T07:39:25.197Z","categories_index":"深度学习","tags_index":"深度学习","author_index":"麟"}]